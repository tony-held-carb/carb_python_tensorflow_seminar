{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TL/DR\n",
    "1. Run entire notebook\n",
    "2. Results saved to: /local/ml_local/obj_detection/images_videos/output\n",
    "3. (Optional) Go to section \"Select a TF model by index for object detection\" and select the index of the pre-trained TF model to use for object detection\n",
    "4. (Optional) Go to section \"Detecting/Visualizing a Single Image\" and change the variable image_path_as_string to change default image to one of your choosing.\n",
    "5. (Optional) Go to section \"Detecting/Visualizing Video\" and change video_input_path to change default video to one of your choosing.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "mE-Y77TR4bVy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "This notebook explores how to use prebuilt TensorFlow models available at Googles TensorFlow Zoo to perform object detection on images, video, and webcams.\n",
    "\n",
    "This notebook will detect objects in an image and video of your choosing.  By default, the image will be a tennis photo and the video will be from Back to the Future.  If you would like to model other images/videos, change the input source in the sections titled \"Detecting/Visualizing a Single Image\" and \"Detecting/Visualizing Video.\"\n",
    "\n",
    "The Open Source Computer Vision (OpenCV) is an open-source BSD-licensed library that includes several hundreds of computer vision algorithms and is used in this tutorial to pre- and post-process images, videos, and streams.  OpenCV is imported into python as cv2.\n",
    "\n",
    "OpenCV uses a different color ordering than the TensorFlow models.  OpenCV represents an image as a 3D numpy array with shape (height, width, 3) where 3 are the colors in order of BGR (blue, green, red). TensorFlow models expect a tensor with shape (batch_size, height, width, 3) with the color order as RGB (red, green, blue). The OpenCV function cv2.cvtColor is used to change the color ordering of OpenCV so that they are compatible with TF.  Some of the TF models require the batch_size to be 1.\n",
    "\n",
    "To avoid confusion between OpenCV images and their associated TF model input, the following naming convention is used:\n",
    "* cv_image: OpenCV image stored as a 3D numpy array with shape (height, width, 3) in BGR format\n",
    "* tf_input: Tensor Flow model input associated with a cv_image stored as a 4D tensor with shape (batch, height, width, 3) in RGB format.\n",
    "\n",
    "* File convention:\n",
    "  * All file references should be of type Path\n",
    "  * Any function that accepts a file reference should convert it to Path\n",
    "  * Paths should be cast to strings when calling 3rd party libraries such as cv2\n",
    "  * cv2.imshow() is disabled in Colab, because it causes Jupyter sessions to crash\n",
    "    * see https://github.com/jupyter/notebook/issues/3935.\n",
    "    * A workaround is to use cv2_imshow rather than cv2.imshow() on colab\n",
    "    * from google.colab.patches import cv2_imshow\n",
    "\n",
    "References:\n",
    "* __[Read and Write Videos using OpenCV](https://arshren.medium.com/read-and-write-videos-using-opencv-7f92548afcba)__\n",
    "* __[Introduction to OpenCV](https://www.geeksforgeeks.org/introduction-to-opencv/)__\n",
    "* __[Object Detection Tutorial](https://www.youtube.com/watch?v=2yQqg_mXuPQ)__\n",
    "\n",
    "Version Notes:\n",
    "* v24: TF models work on Windows Pycharm with GPU support\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "U5WSBMju4bV0"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aopV-dedcLLm",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8lnRmhAJcLLm",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.712052Z",
     "start_time": "2024-04-28T23:42:08.811556Z"
    }
   },
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "from tqdm import tqdm\n",
    "import pprint"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# import object_detection_utils"
   ],
   "metadata": {
    "id": "-ZJdzlBYbzyy",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.728053Z",
     "start_time": "2024-04-28T23:42:10.713053Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TfRJhoQocLLn",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Utility Functions & Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "e46P2AQqcLLn",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AwuD--KccLLn",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.744056Z",
     "start_time": "2024-04-28T23:42:10.729053Z"
    }
   },
   "source": [
    "def cv_image_list_to_tf_input(cv_images):\n",
    "  \"\"\"\n",
    "  Convert a list of cv images into a 4D tensor for TF modeling.\n",
    "\n",
    "  Args:\n",
    "    cv_images:\n",
    "        list of cv images represented as 3D numpy arrays with\n",
    "        shape (height, width, 3) in BGR format\n",
    "\n",
    "  Returns:\n",
    "    4D tensor with shape (batch, height, width, 3) in RGB format with datatype uint8\n",
    "\n",
    "  \"\"\"\n",
    "  cv_images_corrected = []\n",
    "  for cv_image in cv_images:\n",
    "    cv_images_corrected.append(cv2.cvtColor(cv_image.copy(), cv2.COLOR_BGR2RGB))\n",
    "  np_images = np.stack(cv_images_corrected)\n",
    "  tf_input = tf.convert_to_tensor(np_images, dtype=tf.uint8)\n",
    "\n",
    "  return tf_input"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "leeiiwi4cLLn",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.759564Z",
     "start_time": "2024-04-28T23:42:10.745058Z"
    }
   },
   "source": [
    "def image_np_to_tf(numpy_array):\n",
    "  \"\"\"\n",
    "  Convert a numpy array representing an image with shape\n",
    "  (height, width, 3) or (batch, height, width, 3) into a tensor of\n",
    "  shape (batch, height, width, 3) with datatype uint8\n",
    "  which is suitable for TF modeling.\n",
    "\n",
    "  Notes:\n",
    "    1) Color order is not changed\n",
    "    2) If the numpy_array is 3D a dummy batch dimension of 1 is added.\n",
    "    \"\"\"\n",
    "  # Convert np to tf ensuring the correct data type\n",
    "  tf_input = tf.convert_to_tensor(numpy_array, dtype=tf.uint8)\n",
    "  # tf model expects 4D tensors with shape ( batch, height, width, 3)\n",
    "  if len(tf_input.shape) == 4:\n",
    "    pass\n",
    "  # if array is shape (height, width, 3), add a dummy batch axis\n",
    "  elif len(tf_input.shape) == 3:\n",
    "    tf_input = tf_input[tf.newaxis, ...]\n",
    "  else:\n",
    "    msg = f\"Tensor must be either 4D (batch, height, width, color).\" \\\n",
    "          f\"or 3D (height, width, color).  Incompatible shape: {tf_input.shape}\"\n",
    "    raise ValueError()\n",
    "  print(f\"{type(tf_input)=}, {tf_input.shape=}\")\n",
    "  return tf_input"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n80Eg4-VcLLn",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.775563Z",
     "start_time": "2024-04-28T23:42:10.760563Z"
    }
   },
   "source": [
    "def convert_coordinate_system(cv_image, tf_bounding_box):\n",
    "  \"\"\"\n",
    "  Convert tf_bounding_box coordinates that range from 0 to 1 into\n",
    "  pixel coordinates to suitable for cv plotting routines.\n",
    "\n",
    "  Args:\n",
    "    cv_image:\n",
    "      An image with shape (height, width, depth) that will be overlayed\n",
    "      with an object detection bounding box.\n",
    "    tf_bounding_box:\n",
    "      TF model output bounding box that uses a 0 to 1 coordinate system.\n",
    "      The bounding box is in the form (y_min, x_min, y_max, x_max).\n",
    "  Returns:\n",
    "    pixel based coordinate in the form (x_min, x_max, y_min, y_max) suitable for\n",
    "    cv2 plotting routines.\n",
    "  \"\"\"\n",
    "  image_height, image_width, _ = cv_image.shape\n",
    "  y_min, x_min, y_max, x_max = tf_bounding_box\n",
    "  x_min, x_max, y_min, y_max = (x_min * image_width,\n",
    "                                x_max * image_width,\n",
    "                                y_min * image_height,\n",
    "                                y_max * image_height)\n",
    "\n",
    "  return int(x_min), int(x_max), int(y_min), int(y_max)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bJI1hNFLcLLo",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.791563Z",
     "start_time": "2024-04-28T23:42:10.776564Z"
    }
   },
   "source": [
    "def overlay_text(cv_image,\n",
    "                 text,\n",
    "                 font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                 position=(0, 0),\n",
    "                 font_scale=1,\n",
    "                 font_thickness=1,\n",
    "                 rectangle_color=(0, 255, 0),\n",
    "                 ):\n",
    "  \"\"\"\n",
    "  Overlay text on a cv_image.  The text will be white on a rectangle of color rectangle_color.\n",
    "\n",
    "  Notes:\n",
    "  1) The top left of window is the origin, as you increase y, you move downwards on the screen.\n",
    "  2) When you place text, you specify the bottom left position\n",
    "  \"\"\"\n",
    "  # location determines if the text box is inside the bounding box, or above it\n",
    "  # location = 'inside'\n",
    "  location = 'above'\n",
    "\n",
    "  white = (255, 255, 255)\n",
    "  black = (0, 0, 0)\n",
    "  pad = 3  # pads the size of the background box\n",
    "\n",
    "  x, y = position\n",
    "  # Get the width (w) and height (h) of the text to be overlayed\n",
    "  (w, h), _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "\n",
    "  # create background rectangle of class color\n",
    "  # put text over transparent section\n",
    "  if location == 'inside':\n",
    "    cv_image[y:y + h + pad, x:x + w + pad] = rectangle_color\n",
    "    cv2.putText(cv_image, text, (x, y + h), font, font_scale, white, font_thickness)\n",
    "  else:\n",
    "    cv_image[y - h - 4:y, x:x + w] = rectangle_color\n",
    "    cv2.putText(cv_image, text, (x, y - 3), font, font_scale, white, font_thickness)\n",
    "\n",
    "  return w, h"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rax9qI8McLLo",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.807563Z",
     "start_time": "2024-04-28T23:42:10.792563Z"
    }
   },
   "source": [
    "def draw_bounding_box(cv_image, class_color, x_min, x_max, y_min, y_max):\n",
    "  \"\"\"\n",
    "  Draw a bounding box on a cv_image with a specified color.\n",
    "  x and y coordinates are in pixel coordinates (not 0 to 1 format)\n",
    "  \"\"\"\n",
    "  # Draw a thin bounding box around min and max coordinates\n",
    "  cv2.rectangle(cv_image,\n",
    "                pt1=(x_min, y_min),\n",
    "                pt2=(x_max, y_max),\n",
    "                color=class_color,\n",
    "                thickness=1)\n",
    "\n",
    "  # Thicken the bounding box near the corners\n",
    "  lineWidth = min(int((x_max - x_min) * 0.2), int((y_max - y_min) * 0.2))\n",
    "\n",
    "  cv2.line(cv_image, (x_min, y_min), (x_min + lineWidth, y_min), class_color, thickness=5)\n",
    "  cv2.line(cv_image, (x_min, y_min), (x_min, y_min + lineWidth), class_color, thickness=5)\n",
    "\n",
    "  cv2.line(cv_image, (x_max, y_min), (x_max - lineWidth, y_min), class_color, thickness=5)\n",
    "  cv2.line(cv_image, (x_max, y_min), (x_max, y_min + lineWidth), class_color, thickness=5)\n",
    "\n",
    "  cv2.line(cv_image, (x_min, y_max), (x_min + lineWidth, y_max), class_color, thickness=5)\n",
    "  cv2.line(cv_image, (x_min, y_max), (x_min, y_max - lineWidth), class_color, thickness=5)\n",
    "\n",
    "  cv2.line(cv_image, (x_max, y_max), (x_max - lineWidth, y_max), class_color, thickness=5)\n",
    "  cv2.line(cv_image, (x_max, y_max), (x_max, y_max - lineWidth), class_color, thickness=5)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WhwUxlU8cLLo",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.823564Z",
     "start_time": "2024-04-28T23:42:10.808564Z"
    }
   },
   "source": [
    "def annotate_bounding_box(cv_image, detection_box, display_text, class_color):\n",
    "  \"\"\"\n",
    "  Annotate a cv_image with a single object's class name, confidence, and bounding box\n",
    "  in a specified color.\n",
    "\n",
    "  Notes:\n",
    "    1) detection_box coordinate system is the 0 to 1 TF model output format.\n",
    "  \"\"\"\n",
    "  # convert fractional bounding box coordinates (0 to 1) to pixel coordinates\n",
    "  x_min, x_max, y_min, y_max = convert_coordinate_system(cv_image, detection_box)\n",
    "\n",
    "  draw_bounding_box(cv_image, class_color, x_min, x_max, y_min, y_max)\n",
    "\n",
    "  overlay_text(cv_image,\n",
    "               display_text,\n",
    "               font=cv2.FONT_HERSHEY_DUPLEX,\n",
    "               position=(x_min, y_min),\n",
    "               font_scale=1,\n",
    "               font_thickness=2,\n",
    "               rectangle_color=class_color,\n",
    "               )"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_j8B-xV1cLLo",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.839567Z",
     "start_time": "2024-04-28T23:42:10.824563Z"
    }
   },
   "source": [
    "def video_properties(video_capture, cv_image):\n",
    "  \"\"\"\n",
    "  Create a dictionary of key video characteristics associated with a video file.\n",
    "\n",
    "  Args:\n",
    "      video_capture:\n",
    "          Created with cv2.VideoCapture(video_input_path)\n",
    "      cv_image:\n",
    "          An numpy image with shape (height, width, depth)\n",
    "  Returns:\n",
    "      Dictionary with key video properties\n",
    "  \"\"\"\n",
    "  properties = {}\n",
    "  image_height, image_width, _ = cv_image.shape\n",
    "  properties['height'] = image_height\n",
    "  properties['width'] = image_width\n",
    "  properties['fps'] = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "  properties['frames'] = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "  properties['duration_in_seconds'] = round(properties['frames'] / properties['fps'], 2)\n",
    "\n",
    "  return properties"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "32Bj9ajTcLLo",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.855626Z",
     "start_time": "2024-04-28T23:42:10.840567Z"
    }
   },
   "source": [
    "def url_file_name(path_url):\n",
    "  \"\"\"\n",
    "  Return the file name of a url with and without a suffix\n",
    "\n",
    "  Args:\n",
    "  path_url (Path):\n",
    "\n",
    "  Returns:\n",
    "    file_name, file_name_no_suffix\n",
    "\n",
    "  Examples:\n",
    "    1) http://site.org/file_name.tar.gz -> file_name.tar.gz, file_name\n",
    "    2) http://site.org/file_name.tar -> file_name.tar, file_name\n",
    "    3) http://site.org/file_name -> file_name, file_name\n",
    "\n",
    "  \"\"\"\n",
    "  path_url = Path(path_url)\n",
    "  file_name = path_url.name\n",
    "\n",
    "  if '.' in file_name:\n",
    "    first_dot = file_name.index('.')\n",
    "    file_name_no_suffix = file_name[:first_dot]\n",
    "  else:\n",
    "    file_name_no_suffix = file_name\n",
    "\n",
    "  return file_name, file_name_no_suffix"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UaV-bnwScLLo",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.871627Z",
     "start_time": "2024-04-28T23:42:10.856626Z"
    }
   },
   "source": [
    "def output_file_name(model_name,\n",
    "                     file_name_in,\n",
    "                     codec=\"\",\n",
    "                     ):\n",
    "  \"\"\"\n",
    "  Create an output directory (if needed) and suitable file names for model output\n",
    "  and logging.\n",
    "\n",
    "  Args:\n",
    "    model_name (str):\n",
    "      ML model name used for object detection\n",
    "    file_name_in (Path):\n",
    "      image or video file to be processed in the form: input_path/file_name.file_suffix\n",
    "      example:  /home/user/images/input/my_video.mp4\n",
    "    codec:\n",
    "      four letter codec for video encoding\n",
    "\n",
    "  Returns:\n",
    "    image_name_out:\n",
    "      file name to output annotated image or video using cv2\n",
    "      example:  /home/user/images/output/my_video_<tf_model_name>_<codec>.mp4\n",
    "    log_name_out:\n",
    "      file name to output image detection logging information\n",
    "      example:  /home/user/images/output/my_video_<tf_model_name>_<codec>.log\n",
    "\n",
    "  \"\"\"\n",
    "  file_name_in = Path(file_name_in)\n",
    "  input_directory = file_name_in.parent\n",
    "  output_directory = Path.joinpath(input_directory.parent, 'output')\n",
    "\n",
    "  output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "  base_name_out = file_name_in.stem + \"_\" + model_name\n",
    "\n",
    "  if codec:\n",
    "    new_file_name = base_name_out + \"_\" + codec + file_name_in.suffix\n",
    "  else:\n",
    "    new_file_name = base_name_out + file_name_in.suffix\n",
    "\n",
    "  image_name_out = Path.joinpath(output_directory, new_file_name)\n",
    "  log_name_out = Path.joinpath(output_directory, base_name_out + \".log\")\n",
    "  return image_name_out, log_name_out"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2n_3hPSCcLLp",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.887627Z",
     "start_time": "2024-04-28T23:42:10.872627Z"
    }
   },
   "source": [
    "def get_file_by_url(url,\n",
    "                    cache_dir,\n",
    "                    cache_subdir=\"\"):\n",
    "  \"\"\"\n",
    "  Download a url (if not already cached) to local file system\n",
    "\n",
    "  Args:\n",
    "    url (str):\n",
    "      url to download\n",
    "    cache_dir:\n",
    "    cache_subdir:\n",
    "\n",
    "  Returns:\n",
    "    file_path of cached/downloaded file\n",
    "\n",
    "  Notes:\n",
    "    ?raw=true used in github downloads will be stripped off the file_name\n",
    "  \"\"\"\n",
    "  url_path = Path(url)\n",
    "\n",
    "  file_name = url_path.name\n",
    "  raw_suffix = file_name.find('?raw=true')\n",
    "  if raw_suffix != -1:\n",
    "    file_name = file_name[:raw_suffix]\n",
    "\n",
    "  file_path = get_file(file_name,\n",
    "                       origin=url,\n",
    "                       cache_dir=cache_dir,\n",
    "                       cache_subdir=cache_subdir,\n",
    "                       extract=False,\n",
    "                       )\n",
    "  return Path(file_path)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "def get_files_by_url_list(url_list,\n",
    "                          cache_dir):\n",
    "  \"\"\"\n",
    "  Download a list of urls (if not already cached) to local file system\n",
    "  See: get_file_by_url\n",
    "  \"\"\"\n",
    "\n",
    "  file_names = []\n",
    "  for url in url_list:\n",
    "    file_name = get_file_by_url(url=url,\n",
    "                                cache_dir=cache_dir)\n",
    "    file_names.append(file_name)\n",
    "  return file_names"
   ],
   "metadata": {
    "id": "iapJd54w_HMb",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.903628Z",
     "start_time": "2024-04-28T23:42:10.888627Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "def list_files_in_path(path, verbose=0):\n",
    "  \"\"\"\n",
    "  No recursive listing of files in a path\n",
    "\n",
    "  path (Path):\n",
    "  verbose (int):\n",
    "    0 to supress diagnostics\n",
    "    1 to output diagnostics\n",
    "\n",
    "  Returns (dict):\n",
    "    dictionary of file path names in directory\n",
    "  \"\"\"\n",
    "  file_dict = {}\n",
    "  if verbose > 0:\n",
    "    print(f\"File(s) available in {path}\\nindex, file name\\n{'-' * 60}\")\n",
    "\n",
    "  for i, item in enumerate(path.iterdir()):\n",
    "    if item.is_file():\n",
    "      file_dict[i] = item\n",
    "      if verbose > 0:\n",
    "        print(f\"{i}, {item.stem + item.suffix}\")\n",
    "\n",
    "  return file_dict"
   ],
   "metadata": {
    "id": "bE3M85iC_HMb",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.919628Z",
     "start_time": "2024-04-28T23:42:10.905628Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MJp7HrCycLLp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Color Class\n",
    "Assists in making brightly colored bounding box plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xds9ZzsTcLLp",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.935627Z",
     "start_time": "2024-04-28T23:42:10.920628Z"
    }
   },
   "source": [
    "class Colors:\n",
    "  \"\"\"\n",
    "  Utility class to select bright and easy to read colors for bounding boxes.\n",
    "\n",
    "  Usage:\n",
    "    1) colors = Colors()\n",
    "    2) colors.rgb_colors(int_value) -> returns RGB tuple\n",
    "\n",
    "  Notes:\n",
    "      1) Adjust self.hex_colors to change the available colors\n",
    "      2) Colors will cycle if int_value is larger than color list length\n",
    "      3) colors can be returned in either RGB or BGR ordering.\n",
    "\n",
    "  References:\n",
    "      1) https://matplotlib.org/stable/gallery/color/named_colors.html\n",
    "      2) https://github.com/ultralytics/yolov5/issues/670#issuecomment-872348461\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.hex_colors = {'blue': '#1f77b4',\n",
    "                       'orange': '#ff7f0e',\n",
    "                       'green': '#2ca02c',\n",
    "                       'red': '#d62728',\n",
    "                       'cyan': '#17becf',\n",
    "                       'fuchsia': '#FF00FF',\n",
    "                       }\n",
    "    self.rgb_colors = [self.hex2rgb(color) for color in self.hex_colors.values()]\n",
    "    self.bgr_colors = [(b, g, r) for (r, g, b) in self.rgb_colors]\n",
    "\n",
    "  def hex2rgb(self, h):\n",
    "    \"\"\"Convert hex colors to rgb (using PIL ordering)\"\"\"\n",
    "    return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
    "\n",
    "  def get_rgb(self, index):\n",
    "    \"\"\"Return a RGB color tuple\"\"\"\n",
    "    index = index % len(self.rgb_colors)\n",
    "    return self.rgb_colors[index]\n",
    "\n",
    "  def get_bgr(self, index):\n",
    "    \"\"\"Return a BGR color tuple\"\"\"\n",
    "    index = index % len(self.bgr_colors)\n",
    "    return self.bgr_colors[index]\n",
    "\n",
    "  def diagnostics(self, number_of_colors=10):\n",
    "    \"\"\"Output color tuples for diagnostics\"\"\"\n",
    "    for i in range(number_of_colors):\n",
    "      print(f\"Color tuples in RGB: {colors.get_rgb(i)}, BGR: {colors.get_bgr(i)}\")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "x1kixfPqcLLp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Color Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IMTjEtxMcLLp",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.951194Z",
     "start_time": "2024-04-28T23:42:10.936627Z"
    }
   },
   "source": [
    "colors = Colors()\n",
    "colors.diagnostics()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color tuples in RGB: (31, 119, 180), BGR: (180, 119, 31)\n",
      "Color tuples in RGB: (255, 127, 14), BGR: (14, 127, 255)\n",
      "Color tuples in RGB: (44, 160, 44), BGR: (44, 160, 44)\n",
      "Color tuples in RGB: (214, 39, 40), BGR: (40, 39, 214)\n",
      "Color tuples in RGB: (23, 190, 207), BGR: (207, 190, 23)\n",
      "Color tuples in RGB: (255, 0, 255), BGR: (255, 0, 255)\n",
      "Color tuples in RGB: (31, 119, 180), BGR: (180, 119, 31)\n",
      "Color tuples in RGB: (255, 127, 14), BGR: (14, 127, 255)\n",
      "Color tuples in RGB: (44, 160, 44), BGR: (44, 160, 44)\n",
      "Color tuples in RGB: (214, 39, 40), BGR: (40, 39, 214)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ftUwGhgrcLLp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## TensorFlowModel Class\n",
    "Class to specify and load a TensorFlow object detection model\n",
    "that can be used to detect objects in images, video, and webcam streams"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KUG-ostEcLLp",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.966701Z",
     "start_time": "2024-04-28T23:42:10.952194Z"
    }
   },
   "source": [
    "# noinspection PyAttributeOutsideInit,PyMethodMayBeStatic\n",
    "class TensorFlowModel:\n",
    "  \"\"\"\n",
    "  Download and/or load TensorFlow model for object detection.\n",
    "\n",
    "  Notes:\n",
    "    1) TensorFlow 2 Detection Model Zoo:\n",
    "        https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n",
    "    2) Coco dataset used to train TF models\n",
    "        https://cocodataset.org/\n",
    "    3) Coco id's are 1 based (human is id:1), to ensure that the coco.names file agrees with the\n",
    "       TF model indexing, a placeholder line with '__Background__' is used to align indexing.\n",
    "    4) Discussion of Coco label types\n",
    "        https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "        https://saturncloud.io/blog/what-is-tensorflowmodels-and-its-relationship-with-coco/\n",
    "        https://www.tensorflow.org/datasets/catalog/coco\n",
    "        https://arxiv.org/pdf/1405.0312.pdf\n",
    "    5) Some articles indicate coco is 91 objects, while others indicate 90.  It appears that some tensorflow\n",
    "       models were trained with 90 object id's.  object 91 is a hairbrush, which may not be considered in some TF models\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               cache_dir,\n",
    "               cache_subdir=\"checkpoints\",\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Read in Coco class names, TF model Zoo names and urls, and create model directory structure.\n",
    "\n",
    "    Args:\n",
    "      cache_dir:\n",
    "        local directory to store TF models\n",
    "      cache_subdir:\n",
    "        TF model sub directory\n",
    "    \"\"\"\n",
    "    self.cache_dir = Path(cache_dir)\n",
    "    self.cache_subdir = cache_subdir\n",
    "\n",
    "    # Create file structure\n",
    "    os.makedirs(self.cache_dir, exist_ok=True)\n",
    "    self.load_meta_data(cache_dir)\n",
    "\n",
    "  def load_meta_data(self, cache_dir):\n",
    "    \"\"\"\n",
    "    Load COCO class names into an array and available TensorFlow models into a dictionary.\n",
    "\n",
    "    Args:\n",
    "      cache_dir:\n",
    "        local directory to store TF models\n",
    "    \"\"\"\n",
    "    # text file with coco dataset names\n",
    "    coco_class_names_url = \"https://raw.githubusercontent.com/tony-held/public/main/obj_detection/coco_class_names.txt\"\n",
    "    # json file with TF model name and associated url\n",
    "    tf_obj_detection_models_url = \"https://raw.githubusercontent.com/tony-held/public/main/obj_detection/tf_object_detection_models.json\"\n",
    "\n",
    "    # Download metadata files to local drive (if necessary)\n",
    "    self.class_names_path = get_file_by_url(coco_class_names_url,\n",
    "                                            cache_dir=cache_dir,\n",
    "                                            cache_subdir='meta_data',\n",
    "                                            )\n",
    "    self.tf_models_path = get_file_by_url(tf_obj_detection_models_url,\n",
    "                                          cache_dir=cache_dir,\n",
    "                                          cache_subdir='meta_data',\n",
    "                                          )\n",
    "    # Read list of objects that have been pretrained for classification in the coco dataset\n",
    "    self.class_names = self.read_class_name_file(self.class_names_path)\n",
    "    # Read name and associated url for TF object detection models available at the the Model Zoo\n",
    "    self.tf_models = self.read_tf_object_models(self.tf_models_path)\n",
    "\n",
    "  def read_class_name_file(self, class_name_file_path):\n",
    "    with open(class_name_file_path, 'r') as read_file:\n",
    "      return read_file.read().splitlines()\n",
    "\n",
    "  def read_tf_object_models(self, tf_models_path):\n",
    "    with open(tf_models_path, \"r\") as read_file:\n",
    "      return json.load(read_file)\n",
    "\n",
    "  def get_model_and_file_name(self, model_url):\n",
    "    \"\"\"\n",
    "    Return the file name and model name associated with a TF model url in tar/zip format.\n",
    "    \"\"\"\n",
    "    model_url = Path(model_url)\n",
    "\n",
    "    file_name, model_name = url_file_name(model_url)\n",
    "\n",
    "    # file_name = os.path.basename(model_url)  # example, efficientdet_d4_coco17_tpu-32.tar.gz\n",
    "    model_name = file_name[:file_name.index('.')]  # example, efficientdet_d4_coco17_tpu-32\n",
    "    return file_name, model_name\n",
    "\n",
    "  def download_model_url(self, model_url):\n",
    "    \"\"\"\n",
    "    Download a local copy of the TF model associated with a model url (if needed).\n",
    "    \"\"\"\n",
    "    file_name, model_name = self.get_model_and_file_name(model_url)\n",
    "\n",
    "    print(\"Downloading model (if necessary): \" + model_name)\n",
    "    # Downloads a file from a url if it not already in the cache.\n",
    "    get_file(fname=str(file_name),\n",
    "             origin=model_url,\n",
    "             cache_dir=self.cache_dir,\n",
    "             cache_subdir=self.cache_subdir,\n",
    "             extract=True)\n",
    "\n",
    "  def load_model_by_url(self, model_url):\n",
    "    \"\"\"\n",
    "    Set self.model to a local copy of the TF model associated with a model url.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    self.file_name, self.model_name = self.get_model_and_file_name(model_url)\n",
    "\n",
    "    self.download_model_url(model_url)\n",
    "    print(\"Loading model: \" + self.model_name)\n",
    "    print(f\"{'-' * 80}\\nThis may generate warnings and other diagnostics\\n{'-' * 80}\")\n",
    "\n",
    "    model_path = Path.joinpath(self.cache_dir,\n",
    "                               self.cache_subdir,\n",
    "                               self.model_name,\n",
    "                               \"saved_model\")\n",
    "\n",
    "    self.model = tf.saved_model.load(str(model_path))\n",
    "\n",
    "    print(\"Model \" + self.model_name + \" loaded successfully...\")\n",
    "\n",
    "  def number_pretrained_models(self):\n",
    "    return len(self.tf_models)\n",
    "\n",
    "  def list_models(self):\n",
    "    \"\"\"\n",
    "    Print a list of available pre-trained TF models\n",
    "    \"\"\"\n",
    "    print(f\"The following pre-trained TF object detection models are available:\\n{'-' * 80}\")\n",
    "    print(f\"Not all models support all features of this notebook and may need code modification to work\")\n",
    "    for i, key in enumerate(self.tf_models):\n",
    "      print(f\"{i}: <{key}>\")\n",
    "\n",
    "  def load_model_by_name(self, model_name):\n",
    "    \"\"\"\n",
    "    Using the dictionary of model names and urls read in from the TF metadata file,\n",
    "    load in a publicly available TF model.\n",
    "\n",
    "    Args:\n",
    "      model_name (str):\n",
    "        name of TF model.  This is a key to the self.tf_models dict\n",
    "    \"\"\"\n",
    "    model_url = self.tf_models[model_name]\n",
    "    self.load_model_by_url(model_url)\n",
    "\n",
    "  def load_model_by_index(self, index):\n",
    "    \"\"\"\n",
    "    Same as load_model_by_name, except an int index is used to specify the model rather\n",
    "    than the model name.\n",
    "    \"\"\"\n",
    "    if index > self.number_pretrained_models():\n",
    "      raise ValueError(f\"Index <{index}> greater than number \"\n",
    "                       f\"of available models <{self.number_pretrained_models()}> \")\n",
    "    for i, key in enumerate(self.tf_models):\n",
    "      if i == index:\n",
    "        self.load_model_by_name(key)\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Single image object detection and annotation"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YZmbHuuz4bV5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def image_annotation(image,\n",
    "                     tf_model,\n",
    "                     score_threshold=0.5,\n",
    "                     iou_threshold=0.5,\n",
    "                     max_output_size=50,\n",
    "                     verbose=0,\n",
    "                     display_image=True,\n",
    "                     display_time_ms=4000,\n",
    "                     save_to_file=True,\n",
    "                     ):\n",
    "  \"\"\"\n",
    "  Annotate list of cv_images with class names, confidence, and bounding boxes using\n",
    "  a pre-trained tensorflow model to detect objects.\n",
    "\n",
    "  Args:\n",
    "    image (Path or np.array):\n",
    "      Path to a single image for object detection or a np.array representing an image\n",
    "    tf_model:\n",
    "      TF object model to be used for object detection\n",
    "    score_threshold: 0.->1.0\n",
    "      User specified minimum confidence score for object detection\n",
    "    iou_threshold: 0.->1.0\n",
    "      User specified threshold for removing overlapping detections\n",
    "    max_output_size:\n",
    "      max number of objects to visualize\n",
    "    verbose (int):\n",
    "      0 for critical messages\n",
    "      1 for general messages\n",
    "      2 for detailed debugging message\n",
    "    display_image (bool):\n",
    "      True to show image\n",
    "    display_time_ms (int):\n",
    "      number of milliseconds to display an image before closing it automatically\n",
    "    save_to_file (bool):\n",
    "      True to save image to file\n",
    "  Returns:\n",
    "    detection_map (dict):\n",
    "      map with image number as the key, and object name and confidence as a list of tuples\n",
    "\n",
    "  Notes:\n",
    "    1) Some object detection models can only process a single image at a time and require a shape of (1, height, width, 3).  When using models with this constraint your image list must be of length 1.\n",
    "    2) The formulation below converts an image path into a list of image paths that will be length 1, future implementations may explore image lengths greater than 1 for models that support larger batches.\n",
    "\n",
    "  \"\"\"\n",
    "  detection_map = {}\n",
    "\n",
    "  if isinstance(image, Path):\n",
    "    image_path_as_string = str(image)\n",
    "    print(f\"Performing object detection on file: {image_path_as_string}\")\n",
    "    # Convert color image to np array with shape: (height, width, 3)\n",
    "    cv_image = cv2.imread(image_path_as_string)\n",
    "  else:\n",
    "    cv_image = image\n",
    "\n",
    "  image_height, image_width, image_depth = cv_image.shape\n",
    "\n",
    "  if verbose >= 1:\n",
    "    print(f\"Converting cv image into color corrected 4D tensor\")\n",
    "    print(f\"Expected shape of input tensor is (1, {image_height}, {image_width}, {image_depth})\")\n",
    "  tf_input = cv_image_list_to_tf_input([cv_image])\n",
    "\n",
    "  # Use TF model to perform object detections\n",
    "  detections = tf_model.model(tf_input)\n",
    "\n",
    "  # there will only be one image processed per tensor because it hard coded by many TF model formulations\n",
    "  # the index i is zero because the detection lists will only be length 1\n",
    "  i = 0\n",
    "  detection_map[i] = []\n",
    "  detection_classes = detections['detection_classes'][i].numpy().astype(np.int32)\n",
    "  detection_scores = detections['detection_scores'][i].numpy()\n",
    "  detection_boxes = detections['detection_boxes'][i].numpy()\n",
    "\n",
    "  # find indices of object detections that satisfy the\n",
    "  # user specified confidence and overlap thresholds\n",
    "  selected_indices = tf.image.non_max_suppression(detection_boxes,\n",
    "                                                  detection_scores,\n",
    "                                                  max_output_size=max_output_size,\n",
    "                                                  iou_threshold=iou_threshold,\n",
    "                                                  score_threshold=score_threshold)\n",
    "\n",
    "  if verbose >= 1:\n",
    "    print(f\"Number of object detections before suppression low confidence and high overlap: {len(detection_classes)}\")\n",
    "    print(f\"Number of object detections after suppression low confidence and high overlap: {len(selected_indices)}\")\n",
    "  if verbose == 2:\n",
    "    print(f\"{selected_indices=}\")\n",
    "\n",
    "  # Loop through bounding boxes that satisfied thresholds and plot them\n",
    "  # on the underlying image\n",
    "  for j in selected_indices:\n",
    "    # Extract detected objects class, confidence score, and bounding box\n",
    "    detection_class = detection_classes[j]\n",
    "    detection_score = round(100 * detection_scores[j])\n",
    "    detection_box = tuple(detection_boxes[j].tolist())\n",
    "    # print(detection_box, detection_class, detection_score)\n",
    "\n",
    "    # Determine class name, associated color, and text to display\n",
    "    class_label_text = tf_model.class_names[detection_class]\n",
    "    class_color = colors.get_bgr(detection_class)\n",
    "    display_text = '{}: {}%'.format(class_label_text.lower(), detection_score)\n",
    "    if verbose == 2:\n",
    "      print(f\"\\tDetection. {display_text}\")\n",
    "    detection_map[i].append((class_label_text, detection_scores[j]))\n",
    "\n",
    "    annotate_bounding_box(cv_image, detection_box, display_text, class_color)\n",
    "\n",
    "  if save_to_file is True:\n",
    "    image_name_out, _ = output_file_name(tf_model.model_name, image)\n",
    "    print(f\"Creating File: {image_name_out}\")\n",
    "    cv2.imwrite(str(image_name_out), cv_image)\n",
    "\n",
    "  if display_image is True:\n",
    "    # colab needs to use a special function for image display\n",
    "    if is_environ_colab is True:\n",
    "      cv2_imshow(cv_image)\n",
    "    else:\n",
    "      cv2.imshow(\"Result\", cv_image)\n",
    "      cv2.waitKey(display_time_ms)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "  return detection_map"
   ],
   "metadata": {
    "id": "Bbh5BQP3_HMc",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.982701Z",
     "start_time": "2024-04-28T23:42:10.967701Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Video object detection and annotation"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fcs7gTsY4bV5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def video_annotation(tf_model,\n",
    "                     video_input_path,\n",
    "                     video_output_path,\n",
    "                     log_file_path,\n",
    "                     score_threshold=0.5,\n",
    "                     iou_threshold=0.5,\n",
    "                     max_output_size=50,\n",
    "                     codec='mp4v',\n",
    "                     output_fps=None,\n",
    "                     skip_frames=0,\n",
    "                     adjust_fps_for_skip=False,\n",
    "                     preview_annotations=True,\n",
    "                     make_annotated_video=True,\n",
    "                     verbose=0):\n",
    "  \"\"\"\n",
    "  Annotate a video with class names, confidence, and bounding boxes using\n",
    "  a pre-trained tensorflow model to detect objects.\n",
    "\n",
    "  Args:\n",
    "    tf_model:\n",
    "      TF model to be used for object detection\n",
    "    video_input_path:\n",
    "      path of video for object detection\n",
    "    video_output_path:\n",
    "      path of video to be created with object annotations\n",
    "    log_file_path:\n",
    "      object detection results file\n",
    "    score_threshold: 0.->1.0\n",
    "      User specified minimum confidence score for object detection\n",
    "    iou_threshold: 0.->1.0\n",
    "      User specified threshold for removing overlapping detections\n",
    "    max_output_size:\n",
    "      max number of objects to visualize\n",
    "    codec:\n",
    "      4-character code of codec used to compress the frames of the output video\n",
    "      supported codes: https://fourcc.org/codecs.php\n",
    "      'mp4v' and 'x264' both create mp4s, 'x264' seems faster and smaller\n",
    "      note that x264 does not appear to be on colab, so safest to use mp4v\n",
    "    output_fps:\n",
    "      Frame rate (frames per second, fps) of the output video.\n",
    "      If None, the frame rate of the input video will be used.\n",
    "    skip_frames:\n",
    "      The number of frames to skip between captures.\n",
    "      =0 to not skip any frames, x to skip x frames after each capture\n",
    "    adjust_fps_for_skip:\n",
    "      if True, adjust the fps of the output file so that skip_frames does not\n",
    "      result in high speed videos\n",
    "    preview_annotations:\n",
    "      True if you want to see object detection video results graphically during processing\n",
    "    make_annotated_video:\n",
    "      True if you want to create an annotated video with object detection boxes\n",
    "    verbose (int):\n",
    "      0 for critical messages\n",
    "      1 for general messages\n",
    "      2 for detailed debugging messages\n",
    "\n",
    "  Returns:\n",
    "    detection_map (dict):\n",
    "      map with frame number as the key, and object name and confidence as a list of tuples\n",
    "  \"\"\"\n",
    "  start_time = datetime.datetime.now()\n",
    "\n",
    "  # Save arguments for diagnostic\n",
    "  diagnostics = vars()\n",
    "\n",
    "  # dict to store object detection results\n",
    "  detection_map = {}\n",
    "\n",
    "  # Open log file\n",
    "  logger = open(log_file_path, 'w')\n",
    "  print(f\"Logging object detection results to: {log_file_path}\")\n",
    "  logger.write(f\"video_annotation() called with the following arguments:\\n{'-' * 80}\\n\")\n",
    "  pprint.pprint(diagnostics, logger, sort_dicts=False)\n",
    "\n",
    "  # Open video file\n",
    "  video_capture = cv2.VideoCapture(video_input_path)\n",
    "\n",
    "  if (video_capture.isOpened() == False):\n",
    "    msg = f\"Error opening file: {video_input_path}\"\n",
    "    raise IOError(msg)\n",
    "\n",
    "  # capture first video frame and save key video properties to dict\n",
    "  (success, cv_image) = video_capture.read()\n",
    "  video_props = video_properties(video_capture, cv_image)\n",
    "  logger.write(f\"\\nInput video has the following properties\\n{'-' * 80}\\n\")\n",
    "  pprint.pprint(video_props, logger)\n",
    "  logger.write(f\"\\nFrame #, Objects Detected\\n{'-' * 80}\\n\")\n",
    "\n",
    "  # if output frames per second is not set by user, use the input fps\n",
    "  if output_fps is None:\n",
    "    output_fps = video_props['fps']\n",
    "  # adjust outputfile framerate if desired\n",
    "  if adjust_fps_for_skip is True:\n",
    "    output_fps = output_fps / (1 + skip_frames)\n",
    "\n",
    "  # Create video output file writer if desired\n",
    "  if make_annotated_video is True:\n",
    "    # VideoWriter_fourcc expects four single characters as arguments\n",
    "    # *'mp4v' will expand to 'm', 'p', '4', 'v'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    video_out = cv2.VideoWriter(str(video_output_path),\n",
    "                                fourcc,\n",
    "                                output_fps,\n",
    "                                (video_props['width'], video_props['height']))\n",
    "  else:\n",
    "    video_out = None\n",
    "\n",
    "  # counters/timers\n",
    "  frame_number = 0  # frame number of video\n",
    "  frames_modeled = 0  # Number of frames modeled with TF object detection\n",
    "  tf_fps = 0  # The number of frames per second processed by TF object detector\n",
    "  call_time = time.time()\n",
    "\n",
    "  # tqdm creates a completion status bar\n",
    "  with tqdm(total=video_props['frames'],\n",
    "            unit=' frames',\n",
    "            desc='Frames Processed') as pbar:\n",
    "\n",
    "    while success and cv_image is not None:\n",
    "\n",
    "      # print(frame_number)\n",
    "      # start timer for processing speed calculations\n",
    "      start_time = time.time()\n",
    "\n",
    "      # Take a single image, create an single member image list, and run image detection\n",
    "      image_detection_map = image_annotation(cv_image,\n",
    "                                             tf_model,\n",
    "                                             score_threshold=score_threshold,\n",
    "                                             iou_threshold=iou_threshold,\n",
    "                                             max_output_size=max_output_size,\n",
    "                                             save_to_file=False,\n",
    "                                             display_image=False,\n",
    "                                             )\n",
    "      # Save the objects detected into the detection map\n",
    "      # Note, this assumes that image batch sizes are 1, if larger sizes are used\n",
    "      # then this methodology needs to be modified.\n",
    "      # [0] because there is only one image in the image list\n",
    "      detection_map[frame_number] = image_detection_map[0]\n",
    "\n",
    "      msg = f\"{frame_number}, {detection_map[frame_number]}\"\n",
    "      logger.write(msg + '\\n')\n",
    "\n",
    "      fps_caption = \"FPS: \" + str(int(tf_fps))\n",
    "      cv2.putText(img=cv_image,\n",
    "                  text=fps_caption,\n",
    "                  org=(20, 20),\n",
    "                  fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "                  fontScale=2,\n",
    "                  color=(0, 255, 0),  # green\n",
    "                  thickness=2)\n",
    "\n",
    "      if verbose >= 1:\n",
    "        print(f\"{frame_number = }, TF model FPS {tf_fps:.2f}\")\n",
    "\n",
    "      if preview_annotations is True:\n",
    "        if is_environ_colab is True:\n",
    "          # Suppressing image preview on colab as it will strain memory\n",
    "          # cv2_imshow(cv_images[0])\n",
    "          pass\n",
    "        else:\n",
    "          cv2.imshow(\"Result\", cv_image)\n",
    "          # cv2.waitKey(milliseconds) display the frame for given milliseconds or until any key is pressed\n",
    "          # 0xFF is a bit filter that takes only the last 8 bits of the key pressed\n",
    "          key = cv2.waitKey(1) & 0xFF\n",
    "          if key == ord(\"q\"):\n",
    "            success = False\n",
    "            break\n",
    "\n",
    "      if make_annotated_video is True:\n",
    "        video_out.write(cv_image)\n",
    "\n",
    "      # skip interim frames\n",
    "      for j in range(skip_frames):\n",
    "        frame_number += 1\n",
    "        video_capture.grab()\n",
    "\n",
    "      # read next frame\n",
    "      frame_number += 1\n",
    "      (success, cv_image) = video_capture.read()\n",
    "\n",
    "      # update counters and calculate frame rate\n",
    "      end_time = time.time()\n",
    "      tf_fps = 1 / (end_time - start_time)\n",
    "      pbar.update(1 + skip_frames)\n",
    "\n",
    "  if make_annotated_video is True:\n",
    "    video_out.release()\n",
    "    print(f\"\\nCreating file: {video_output_path}\")\n",
    "\n",
    "  end_time = time.time()\n",
    "  msg = f\"Video processing complete in {end_time - call_time:.2f} seconds.\"\n",
    "  print(msg)\n",
    "  logger.write(msg + '\\n')\n",
    "  logger.close()\n",
    "\n",
    "  return detection_map"
   ],
   "metadata": {
    "id": "LONVscyt4bV5",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:10.998701Z",
     "start_time": "2024-04-28T23:42:10.983701Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Determine Runtime Environment and Project Directory Structure"
   ],
   "metadata": {
    "collapsed": false,
    "id": "0aYOccWp4bV5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Determine machine operating system"
   ],
   "metadata": {
    "collapsed": false,
    "id": "q0e-9oK14bV5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Determine machine type\n",
    "# path_prefix is used to specify a drive letter (windows only)\n",
    "if os.name == 'posix':\n",
    "  print(f\"Running on a linux/mac system\")\n",
    "elif os.name == 'nt':\n",
    "  print(f\"Running on a windows system\")\n",
    "else:\n",
    "  raise ValueError('Unknown operating system')"
   ],
   "metadata": {
    "id": "wTUPupGx4bV5",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:11.013700Z",
     "start_time": "2024-04-28T23:42:10.999701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on a windows system\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Determine if environment is Colab\n",
    "\n",
    "Notes:\n",
    "* Image processing with cv2 on Colab has some quirks and requires workarounds\n",
    "* For details, see the following refs:\n",
    "  * __[Introduction to Image Processing in Python](https://colab.research.google.com/github/xn2333/OpenCV/blob/master/Image_Processing_in_Python_Final.ipynb#scrollTo=OU4AAstR0HG6)__\n",
    "  * __[cv2imshow doesnt render video file in Google Colab](https://saturncloud.io/blog/cv2imshow-doesnt-render-video-file-in-google-colab/)__"
   ],
   "metadata": {
    "collapsed": false,
    "id": "vh4rXs1m4bV6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "colab_release_tag = os.getenv(\"COLAB_RELEASE_TAG\")\n",
    "if colab_release_tag:\n",
    "  print(f\"Colab Release Tag: {colab_release_tag}\")\n",
    "  is_environ_colab = True\n",
    "  from google.colab.patches import cv2_imshow  # required for image display on colab\n",
    "else:\n",
    "  is_environ_colab = False\n",
    "\n",
    "print(f\"Is runtime Google Colab: {is_environ_colab}\")"
   ],
   "metadata": {
    "id": "IAuZ21sq4bV6",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:11.028701Z",
     "start_time": "2024-04-28T23:42:11.014701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is runtime Google Colab: False\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specify file structure for input and TF model location\n",
    "\n",
    "Notes:\n",
    "* obj_detection_path and path_prefix determine the base directory for obj detection modeling input/output\n",
    "* obj_detection_path is a string using unix (forward slash) notation.  Use the forward slash notation even if working on windows\n",
    "* path_prefix only applies to windows lets you specify the drive letter of the obj_detection_path\n",
    "* You can change obj_detection_path and path_prefix, but it is recommended that your directory should not be in a directory that is synced with online back-up system (OneDrive, GoogleDrive, etc)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "VWHq3P0h4bV6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "obj_detection_path = \"/local/ml_local/obj_detection/\"\n",
    "path_prefix = \"D:\"  # Only applies to windows, ignored on other platforms\n",
    "\n",
    "if os.name != 'nt':\n",
    "  path_prefix = \"\"\n",
    "\n",
    "# Location of input images and videos to be processed\n",
    "image_input_directory = Path(path_prefix + obj_detection_path + \"images_videos/input\")\n",
    "image_input_directory.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Location of input images and videos: {image_input_directory}\")\n",
    "\n",
    "# Location of pretrained models for object detection\n",
    "pretrained_model_directory = Path(path_prefix + obj_detection_path + \"pretrained_models\")\n",
    "pretrained_model_directory.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Location of pre-trained object detection models: {pretrained_model_directory}\")"
   ],
   "metadata": {
    "id": "YpTFDCdI4bV6",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:11.044706Z",
     "start_time": "2024-04-28T23:42:11.029701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location of input images and videos: D:\\local\\ml_local\\obj_detection\\images_videos\\input\n",
      "Location of pre-trained object detection models: D:\\local\\ml_local\\obj_detection\\pretrained_models\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M511LOo9XlhC"
   },
   "source": [
    "# Download images/video for processing\n",
    "Download example images/videos from github for example purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YxaRs6anXkhJ",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:11.384162Z",
     "start_time": "2024-04-28T23:42:11.045706Z"
    }
   },
   "source": [
    "url_list = [\"https://github.com/tony-held/public/blob/main/obj_detection/tennis_01.jpg?raw=true\",\n",
    "            \"https://github.com/tony-held/public/blob/main/obj_detection/animals_01.jpg?raw=true\",\n",
    "            \"https://github.com/tony-held/public/blob/main/obj_detection/back_to_the_future_skate.mp4?raw=true\",\n",
    "            ]\n",
    "file_names = get_files_by_url_list(url_list,\n",
    "                                   image_input_directory)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "List files available in local image_input_directory"
   ],
   "metadata": {
    "collapsed": false,
    "id": "0oVYgoW7_HMc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "file_dict = list_files_in_path(path=image_input_directory, verbose=1)"
   ],
   "metadata": {
    "id": "CiYNEpEx_HMf",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:11.400161Z",
     "start_time": "2024-04-28T23:42:11.385166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File(s) available in D:\\local\\ml_local\\obj_detection\\images_videos\\input\n",
      "index, file name\n",
      "------------------------------------------------------------\n",
      "0, animals_01.jpg\n",
      "1, back_to_the_future.mp4\n",
      "2, back_to_the_future_skate.mp4\n",
      "3, dana_golf.mp4\n",
      "4, four_dogs.jpg\n",
      "5, ian_jumping.mp4\n",
      "6, mountain_bike_01.jpg\n",
      "7, mountain_bike_02.jpg\n",
      "8, mountain_bike_03.jpg\n",
      "9, mountain_bike_04.jpg\n",
      "11, soccer_01.jpg\n",
      "12, street1.mp4\n",
      "13, street2.mp4\n",
      "14, tennis_01.jpg\n",
      "15, your_the_one_that_i_want.mp4\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_B9BtRPJcLLp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load Pretrained TF Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h8ZleX5-cLLp",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:11.416162Z",
     "start_time": "2024-04-28T23:42:11.401162Z"
    }
   },
   "source": [
    "# A TensorFlow model wrapper class allows the selection of a\n",
    "# specific tf object detection model\n",
    "tf_model = TensorFlowModel(cache_dir=pretrained_model_directory)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-x_OxCDhcLLp",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:11.431162Z",
     "start_time": "2024-04-28T23:42:11.417162Z"
    }
   },
   "source": [
    "tf_model.list_models()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following pre-trained TF object detection models are available:\n",
      "--------------------------------------------------------------------------------\n",
      "Not all models support all features of this notebook and may need code modification to work\n",
      "0: <CenterNet HourGlass104 512x512>\n",
      "1: <CenterNet HourGlass104 Keypoints 512x512>\n",
      "2: <CenterNet HourGlass104 1024x1024>\n",
      "3: <CenterNet HourGlass104 Keypoints 1024x1024>\n",
      "4: <CenterNet Resnet50 V1 FPN 512x512>\n",
      "5: <CenterNet Resnet50 V1 FPN Keypoints 512x512>\n",
      "6: <CenterNet Resnet101 V1 FPN 512x512>\n",
      "7: <CenterNet Resnet50 V2 512x512>\n",
      "8: <CenterNet Resnet50 V2 Keypoints 512x512>\n",
      "9: <CenterNet MobileNetV2 FPN 512x512>\n",
      "10: <CenterNet MobileNetV2 FPN Keypoints 512x512>\n",
      "11: <EfficientDet D0 512x512>\n",
      "12: <EfficientDet D1 640x640>\n",
      "13: <EfficientDet D2 768x768>\n",
      "14: <EfficientDet D3 896x896>\n",
      "15: <EfficientDet D4 1024x1024>\n",
      "16: <EfficientDet D5 1280x1280>\n",
      "17: <EfficientDet D6 1280x1280>\n",
      "18: <EfficientDet D7 1536x1536>\n",
      "19: <SSD MobileNet v2 320x320>\n",
      "20: <SSD MobileNet V1 FPN 640x640>\n",
      "21: <SSD MobileNet V2 FPNLite 320x320>\n",
      "22: <SSD MobileNet V2 FPNLite 640x640>\n",
      "23: <SSD ResNet50 V1 FPN 640x640 (RetinaNet50)>\n",
      "24: <SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)>\n",
      "25: <SSD ResNet101 V1 FPN 640x640 (RetinaNet101)>\n",
      "26: <SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)>\n",
      "27: <SSD ResNet152 V1 FPN 640x640 (RetinaNet152)>\n",
      "28: <SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)>\n",
      "29: <Faster R-CNN ResNet50 V1 640x640>\n",
      "30: <Faster R-CNN ResNet50 V1 1024x1024>\n",
      "31: <Faster R-CNN ResNet50 V1 800x1333>\n",
      "32: <Faster R-CNN ResNet101 V1 640x640>\n",
      "33: <Faster R-CNN ResNet101 V1 1024x1024>\n",
      "34: <Faster R-CNN ResNet101 V1 800x1333>\n",
      "35: <Faster R-CNN ResNet152 V1 640x640>\n",
      "36: <Faster R-CNN ResNet152 V1 1024x1024>\n",
      "37: <Faster R-CNN ResNet152 V1 800x1333>\n",
      "38: <Faster R-CNN Inception ResNet V2 640x640>\n",
      "39: <Faster R-CNN Inception ResNet V2 1024x1024>\n",
      "40: <Mask R-CNN Inception ResNet V2 1024x1024>\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qiLfI1amcLLp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Select a TF model by index for object detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0lxxgiRpcLLp",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:41.318705Z",
     "start_time": "2024-04-28T23:42:11.432162Z"
    }
   },
   "source": [
    "# 15: <EfficientDet D4 1024x1024> COCO (mAP): 48.5, speed (ms): 133,  gives excellent results\n",
    "# 13: <EfficientDet D2 768x768>   COCO (mAP): 41.8, speed (ms): 67, twice as fast, lower accuracy\n",
    "# 6: <CenterNet Resnet101 V1 FPN 512x512> COCO (mAP): 34.2, speed (ms): 34, fastest model with mAP>30\n",
    "tf_model.load_model_by_index(15)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model (if necessary): efficientdet_d4_coco17_tpu-32\n",
      "Loading model: efficientdet_d4_coco17_tpu-32\n",
      "--------------------------------------------------------------------------------\n",
      "This may generate warnings and other diagnostics\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___46929) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model efficientdet_d4_coco17_tpu-32 loaded successfully...\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FjcuisqAcLLq",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Object Detection Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "sYD3IEJucLLq",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Detecting/Visualizing a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Change variable below to the image path of your choosing\n",
    "image_path = Path.joinpath(image_input_directory, 'tennis_01.jpg')"
   ],
   "metadata": {
    "id": "W_CP-1Lg4bV7",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:41.334660Z",
     "start_time": "2024-04-28T23:42:41.319705Z"
    }
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kFYQfgpJcLLr",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:56.306008Z",
     "start_time": "2024-04-28T23:42:41.335666Z"
    }
   },
   "source": [
    "detection_map = image_annotation(image_path,\n",
    "                                 tf_model,\n",
    "                                 score_threshold=0.5,\n",
    "                                 iou_threshold=0.5,\n",
    "                                 max_output_size=50,\n",
    "                                 verbose=0,\n",
    "                                 display_time_ms=3000,\n",
    "                                 )\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Classifier type and confidence of detected objects:\\n{detection_map}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing object detection on file: D:\\local\\ml_local\\obj_detection\\images_videos\\input\\tennis_01.jpg\n",
      "Creating File: D:\\local\\ml_local\\obj_detection\\images_videos\\output\\tennis_01_efficientdet_d4_coco17_tpu-32.jpg\n",
      "Classifier type and confidence of detected objects:\n",
      "{0: [('person', 0.9126301), ('tennis racket', 0.8685208), ('sports ball', 0.83457816), ('person', 0.58147424)]}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7ooxyRF9cLLr",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Detecting/Visualizing Video\n",
    "Select video for object detection by specifying video_input_path.\n",
    "* The default image/video input directory is:\n",
    "  * /local/ml_local/obj_detection/images_videos/input/\n",
    "* back_to_the_future_skate.mp4 will be in the image directory\n",
    "* populate video_input_paths dictionary with other movies you place in your input directory for subsequent modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select input file and codec"
   ],
   "metadata": {
    "collapsed": false,
    "id": "qLTr5kVq_HMg"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JXRuD7u8cLLr",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:56.322126Z",
     "start_time": "2024-04-28T23:42:56.307007Z"
    }
   },
   "source": [
    "video_input_path = Path.joinpath(image_input_directory, 'back_to_the_future_skate.mp4')\n",
    "codec = 'mp4v'  # alternatives are 'xvid' and 'x264'"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Automatic output/logging and diagnostics"
   ],
   "metadata": {
    "collapsed": false,
    "id": "c_Zq45RT_HMg"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iUC_KYdrcLLr",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:42:56.338129Z",
     "start_time": "2024-04-28T23:42:56.323127Z"
    }
   },
   "source": [
    "video_input_path = str(video_input_path)\n",
    "print(f\"Performing object detection on file: {video_input_path}\")\n",
    "video_output_path, log_file_path = output_file_name(tf_model.model_name,\n",
    "                                                    video_input_path,\n",
    "                                                    codec=codec)\n",
    "print(f\"Video input path: {video_input_path}\")\n",
    "print(f\"Video output path: {video_output_path}\")\n",
    "print(f\"Video log file path: {log_file_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing object detection on file: D:\\local\\ml_local\\obj_detection\\images_videos\\input\\back_to_the_future_skate.mp4\n",
      "Video input path: D:\\local\\ml_local\\obj_detection\\images_videos\\input\\back_to_the_future_skate.mp4\n",
      "Video output path: D:\\local\\ml_local\\obj_detection\\images_videos\\output\\back_to_the_future_skate_efficientdet_d4_coco17_tpu-32_mp4v.mp4\n",
      "Video log file path: D:\\local\\ml_local\\obj_detection\\images_videos\\output\\back_to_the_future_skate_efficientdet_d4_coco17_tpu-32.log\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nI-RXs4LcLLr",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.404939Z",
     "start_time": "2024-04-28T23:42:56.339126Z"
    }
   },
   "source": [
    "detection_map = video_annotation(tf_model,\n",
    "                                 str(video_input_path),\n",
    "                                 video_output_path=str(video_output_path),\n",
    "                                 log_file_path=log_file_path,\n",
    "                                 score_threshold=0.25,  # consider 0.25\n",
    "                                 iou_threshold=0.25,  # consider 0.45\n",
    "                                 max_output_size=50,\n",
    "                                 codec=codec,\n",
    "                                 skip_frames=0,  # consider 5 to skip every 5 frames\n",
    "                                 adjust_fps_for_skip=True,\n",
    "                                 preview_annotations=True,\n",
    "                                 verbose=0)\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging object detection results to: D:\\local\\ml_local\\obj_detection\\images_videos\\output\\back_to_the_future_skate_efficientdet_d4_coco17_tpu-32.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frames Processed: 100%|| 764/764 [03:37<00:00,  3.51 frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating file: D:\\local\\ml_local\\obj_detection\\images_videos\\output\\back_to_the_future_skate_efficientdet_d4_coco17_tpu-32_mp4v.mp4\n",
      "Video processing complete in 217.91 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Details and Diagnostics of TF Model Output\n",
    "This section explores the general modeling process and detailed results\n",
    "for a single hard coded image to allow developers to more easily create\n",
    "custom functions to process TF model output."
   ],
   "metadata": {
    "collapsed": false,
    "id": "JxO8iQyQ4bV8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "image_path = Path.joinpath(image_input_directory, 'tennis_01.jpg')\n",
    "image_path_as_string = str(image_path)\n",
    "print(f\"Performing object detection on file: {image_path_as_string}\")"
   ],
   "metadata": {
    "id": "WJ7uA3-b4bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.420940Z",
     "start_time": "2024-04-28T23:46:34.405939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing object detection on file: D:\\local\\ml_local\\obj_detection\\images_videos\\input\\tennis_01.jpg\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert image file to a np array with shape: (height, width, 3)\n",
    "image = cv2.imread(image_path_as_string)\n",
    "# Change color ordering to be consistent with TF model expectations\n",
    "np_image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB)\n",
    "# Convert corrected color ordering image in numpy format\n",
    "# to a tensor of shape (1, height, width, 3)\n",
    "input_tensor = image_np_to_tf(np_image)"
   ],
   "metadata": {
    "id": "9qfkJ7yc4bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.436446Z",
     "start_time": "2024-04-28T23:46:34.421941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(tf_input)=<class 'tensorflow.python.framework.ops.EagerTensor'>, tf_input.shape=TensorShape([1, 489, 870, 3])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find Objects Using TF Model\n",
    "\n",
    "TF modeling returns a dictionary of tensors.\n",
    "\n",
    "A brief description of the key and the shape of its associated value is shown below.\n",
    "\n",
    "Note that batch_size is the number of images evaluated simultaneously, for a single image, the batch_size = 1\n",
    "\n",
    "* num_detections: shape(batch_size,)\n",
    "  * the number of objects detected\n",
    "* detection_classes: shape(batch_size, num_detections)\n",
    "  * class number associated with object\n",
    "* detection_scores: shape(batch_size, num_detections)\n",
    "  * confidence of the highest ranked class being correctly identified\n",
    "* detection_multiclass_scores: shape(batch_size, num_detections, num_classes)\n",
    "  * confidence of all class types being correctly identified\n",
    "* detection_boxes: shape(batch_size, num_detections, 4)\n",
    "  * coordinates of bounding boxes\n",
    "* raw_detection_scores: shape(batch_size, raw_num_detections, num_classes)\n",
    "  * all possible raw object detections (before gleaning the reasonable ones)\n",
    "* raw_detection_boxes: shape(batch_size, raw_num_detections, 4)\n",
    "  * all possible raw object detections bounding boxes\n",
    "* detection_anchor_indices\n",
    "\n",
    "The coco dataset is 91 classes, but some tensorflow models appear to use only 90 classes.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4rR5uqmS4bV8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# The following line performs object detection using the specified TF model\n",
    "detections = tf_model.model(input_tensor)"
   ],
   "metadata": {
    "id": "nB03J-534bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.716957Z",
     "start_time": "2024-04-28T23:46:34.437446Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Diagnostics of the model results dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8EXUbnrw4bV8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for key, value in sorted(detections.items()):\n",
    "  print(f\"{key}\\n{value.shape}\\n{'-' * 30}\")"
   ],
   "metadata": {
    "id": "woszKKEb4bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.732517Z",
     "start_time": "2024-04-28T23:46:34.718012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection_anchor_indices\n",
      "(1, 100)\n",
      "------------------------------\n",
      "detection_boxes\n",
      "(1, 100, 4)\n",
      "------------------------------\n",
      "detection_classes\n",
      "(1, 100)\n",
      "------------------------------\n",
      "detection_multiclass_scores\n",
      "(1, 100, 90)\n",
      "------------------------------\n",
      "detection_scores\n",
      "(1, 100)\n",
      "------------------------------\n",
      "num_detections\n",
      "(1,)\n",
      "------------------------------\n",
      "raw_detection_boxes\n",
      "(1, 196416, 4)\n",
      "------------------------------\n",
      "raw_detection_scores\n",
      "(1, 196416, 90)\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Number of significant object detections {detections['num_detections']}\")\n",
    "if 'raw_detection_scores' in detections:\n",
    "  print(f\"Number of raw object detections {detections['raw_detection_scores'].shape}\")"
   ],
   "metadata": {
    "id": "iZow7RK34bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.748517Z",
     "start_time": "2024-04-28T23:46:34.733517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant object detections [100.]\n",
      "Number of raw object detections (1, 196416, 90)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Class ID's associated with the significant object detections.\")\n",
    "detections['detection_classes']"
   ],
   "metadata": {
    "id": "RH0xBQQb4bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.764516Z",
     "start_time": "2024-04-28T23:46:34.749518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ID's associated with the significant object detections.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
       "array([[ 1., 43., 37.,  1., 37.,  1.,  1., 37.,  1., 53.,  1.,  1.,  1.,\n",
       "        34.,  1., 37., 37., 40.,  1., 37., 32.,  1., 57., 47., 86., 85.,\n",
       "         1., 43., 55., 43.,  1.,  1., 43., 77., 41., 43., 74., 60., 39.,\n",
       "        38., 37., 37., 44., 53.,  2., 37.,  1., 34., 44., 10., 37., 34.,\n",
       "         1., 37., 43., 13.,  1., 25., 22., 43., 77., 14., 43.,  1.,  1.,\n",
       "         3., 15., 43., 16.,  3.,  1., 37., 86.,  1., 28., 10.,  1.,  1.,\n",
       "        37., 37.,  1., 32.,  1., 37., 43., 37., 37., 40.,  1.,  1., 37.,\n",
       "        85., 31., 43., 31.,  6., 37., 31., 37.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Confidence (0 to 1) for the most likely class associated with the significant object detections.\")\n",
    "detections['detection_scores'].shape"
   ],
   "metadata": {
    "id": "KpAzJxu04bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.779517Z",
     "start_time": "2024-04-28T23:46:34.765517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence (0 to 1) for the most likely class associated with the significant object detections.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "if 'detection_multiclass_scores' in detections:\n",
    "  print(f\"Confidence (0 to 1) for all classes associated with the significant object detections.\")\n",
    "  print(detections['detection_multiclass_scores'].shape)"
   ],
   "metadata": {
    "id": "_SkOYW7j4bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.795517Z",
     "start_time": "2024-04-28T23:46:34.780517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence (0 to 1) for all classes associated with the significant object detections.\n",
      "(1, 100, 90)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "id": "nmloc5DL4bV8",
    "ExecuteTime": {
     "end_time": "2024-04-28T23:46:34.811520Z",
     "start_time": "2024-04-28T23:46:34.798517Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
